# ml
ml class at the University of British Columbia 

#### jan - apr 2019

##### convolution (1d, 2d)
[convolution 1d](http://www.songho.ca/dsp/convolution/convolution.html#cpp_conv1d)\
[convolution 2d](http://www.songho.ca/dsp/convolution/convolution2d_example.html )

##### map
[map](https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php)

##### basics
[basics of machine learning](https://leetcode.com/explore/learn/card/machine-learning-101/287/what_is_ml/1617/)\
[machine learning algorithm](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)\
[convexity](http://www.ee.bgu.ac.il/~haimp/it/lectures/append2_convex/ConvexFunctions.pdf)

##### algorithms

##### k-Nearest Neighbors

Pros: High accuracy, insensitive to outliers, no assumptions about data

Cons: Computationally expensive, requires a lot of memory
Works with: Numeric values, nominal values

##### Decision trees

Pros: Computationally cheap to use, easy for humans to understand learned results,
missing values OK, can deal with irrelevant features. 
The machine learning appears in how the decision tree is built
The learning database can be a Tree saved as a text file. That data represents the learning base of
the application. Then that training set can be used to classify the data that we want.

Cons: Prone to overfitting
Works with: Numeric values, nominal values


# ml
ml class at the University of British Columbia 

#### jan - apr 2019

##### convolution (1d, 2d)
[convolution 1d](http://www.songho.ca/dsp/convolution/convolution.html#cpp_conv1d)\
[convolution 2d](http://www.songho.ca/dsp/convolution/convolution2d_example.html )

##### map
[map](https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php)

##### basics
[basics of machine learning](https://leetcode.com/explore/learn/card/machine-learning-101/287/what_is_ml/1617/)\
[machine learning algorithm](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)\
[convexity](http://www.ee.bgu.ac.il/~haimp/it/lectures/append2_convex/ConvexFunctions.pdf)

##### algorithms

##### Linear Algebra library for C++ developers

Most Machine learning students use Python to 'learn' machine learning, and Artificial Intelligence in
general. The truth is, Python doesn't look like a build a deep understanding of the nature of
data structures and algorithms used. I am using C++ instead (Find your Java library if you're a Java developer)

http://arma.sourceforge.net/docs.html#eye_standalone


##### k-Nearest Neighbors

Pros: High accuracy, insensitive to outliers, no assumptions about data

Cons: Computationally expensive, requires a lot of memory
Works with: Numeric values, nominal values

##### Decision trees

Pros: Computationally cheap to use, easy for humans to understand learned results,
missing values OK, can deal with irrelevant features. 
The machine learning appears in how the decision tree is built
The learning database can be a Tree saved as a text file. That data represents the learning base of
the application. Then that training set can be used to classify the data that we want.

Cons: Prone to overfitting
Works with: Numeric values, nominal values

##### Naive Bayes

Bayes rule :

P(c,x) = P(x, c) P(c) / P(x)

for a N features problem, we need N^10 samples

##### logistic regression

Another classification method using a function approximation.
